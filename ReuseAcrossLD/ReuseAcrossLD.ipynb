{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from utils import MatrixIO, FileUtils\n",
    "import re, sys, json, os, glob\n",
    "from operator import itemgetter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "mfio = MatrixIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_prefix_list(node_list):\n",
    "    '''Condition can be a lambda function, e.g. lambda xparts: True if xparts[3] != \"bio2rdf.org\" else False'''\n",
    "    pref_types = {}\n",
    "    for node in node_list:\n",
    "        nparts = re.split(\"[#:/]\", node)\n",
    "        prefix = node[:len(node)-len(nparts[len(nparts)-1])]\n",
    "        if not prefix in pref_types: pref_types[prefix] = []\n",
    "        pref_types[prefix].append(node)\n",
    "    return pref_types\n",
    "\n",
    "first_cap_re = re.compile('(.)([A-Z][a-z]+)')\n",
    "all_cap_re = re.compile('([a-z0-9])([A-Z])')\n",
    "\n",
    "def parse_camel_case(name):\n",
    "    s1 = first_cap_re.sub(r'\\1_\\2', name)\n",
    "    return all_cap_re.sub(r'\\1_\\2', s1).lower()\n",
    "\n",
    "def parse_uri_token(token):\n",
    "    prop = parse_camel_case(token)\n",
    "    pparts = re.split(\"[-_]\", prop)\n",
    "    name = \" \".join([x.title() for x in pparts])\n",
    "    return name\n",
    "\n",
    "def parseURI(uri):\n",
    "    nparts = re.split(\"[#:/]\", uri)\n",
    "    name = parse_uri_token(nparts[len(nparts)-1])\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _fileset(folder_path, exclusion_criteria=None):\n",
    "    \"A class of navigators on the file system\"\n",
    "    fileset = []\n",
    "    fpath = \"/Users/mkamdar/Desktop/PhD/lod_query/\"\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for x in dirs:\n",
    "            _f = glob.glob(fpath + folder_path + x + \"/str_inst_df.tsv\") #*_inst_df.tsv\n",
    "            fileset.extend(_f)\n",
    "    return fileset\n",
    "\n",
    "def get_samp_insts(f):\n",
    "    br = pd.read_csv(f, sep=\"\\t\")\n",
    "    brir = br[br[\"dtype\"] == \"iri\"]\n",
    "    samp_insts = list(brir[\"samp_insts\"])\n",
    "    #print samp_insts\n",
    "    act = []\n",
    "    for k in samp_insts:\n",
    "        act.extend(eval(k))\n",
    "    return act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ontoSet = mfio.load_matrix(\"LDschemaontoSet.dat\")\n",
    "vocab_elem = mfio.load_matrix(\"vocab_elem.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fu = FileUtils()\n",
    "mfio = MatrixIO()\n",
    "dirn = [\"inst_exp/\", \"inst_exp2/\", \"inst_exp3/\"]\n",
    "dsetin = {}\n",
    "for m in dirn:\n",
    "    fsets = _fileset(m)\n",
    "    for k in fsets:\n",
    "        dnamep = k.split(\"/\")\n",
    "        print k, dnamep[len(dnamep)-2]\n",
    "        act = get_samp_insts(k)\n",
    "        dsetin[dnamep[len(dnamep)-2]] = act\n",
    "\n",
    "mfio.save_matrix(dsetin, \"dsetin.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totin = []\n",
    "for k in dsetin:\n",
    "    for m in dsetin[k]:\n",
    "        totin.append(m)\n",
    "int_preflist = generate_prefix_list(totin)\n",
    "sortint_preflist = sorted({k: len(int_preflist[k]) for k in int_preflist}.items(), key=itemgetter(1), reverse=True)\n",
    "for k in sortint_preflist: print k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in int_preflist:\n",
    "    if k in disAllPrefs: print k, len(int_preflist[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mfio.save_matrix(sortint_preflist, \"sortint_preflist.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compSet = ontoSet\n",
    "for m in lpreflist: \n",
    "    compSet[m] = {\"dsets\": set([]), \"clcount\": len(lpreflist[m]), 'type': 'unpublishedVocab'}\n",
    "    for k in lpreflist[m]:\n",
    "        if not redG.has_node(k) and k in dsetids:\n",
    "            _dset = dsetids[k]\n",
    "            compSet[m][\"dsets\"] = compSet[m][\"dsets\"].union(_dset)\n",
    "        elif redG.has_node(k):\n",
    "            compSet[m][\"dsets\"] = compSet[m][\"dsets\"].union(set([a.lower() for a in redG.node[k][\"dsets\"].split(\":-:\")]))\n",
    "        elif combG.has_node(k):\n",
    "            compSet[m][\"dsets\"] = compSet[m][\"dsets\"].union(set([a.lower() for a in combG.node[k][\"dsets\"].split(\":-:\")]))\n",
    "print len(compSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for k in compSet:\n",
    "    compSet[k][\"dsets\"] = len(compSet[k][\"dsets\"])\n",
    "    print k, compSet[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lov_df_con = pd.read_csv(\"allVocab_terms.csv\", quotechar='\"', skipinitialspace=True)\n",
    "tot_size = lambda x: lov_df_con[lov_df_con[x[0]].apply(lambda m: True if x[1][0:len(x[1])-1] in m else False)].shape\n",
    "tot_size((\"pURI \", 'uberon'))\n",
    "tot_size_dict = {}\n",
    "for k in vocab_elem.keys(): tot_size_dict[k.strip()] = tot_size((\"vocab \", k))\n",
    "print len(tot_size_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compSet_df = pd.DataFrame.from_dict(compSet, orient=\"index\")\n",
    "compSet_df = compSet_df.reset_index()\n",
    "compSet_df = compSet_df.sort_values(\"clcount\", ascending=False)\n",
    "compSet_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compSet_df.to_csv(\"compSet_df_m.tsv\", sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dset_list = {}\n",
    "dset_count = {}\n",
    "for k in redG.nodes():\n",
    "    if redG.node[k][\"type\"] == \"ontoNode\": continue\n",
    "    dsets = set(redG.node[k][\"dsets\"].split(\":-:\")) - set([\"\"])\n",
    "    if not len(dsets) in dset_count: dset_count[len(dsets)] = set([])\n",
    "    dset_count[len(dsets)].add(k)\n",
    "    for m in dsets:\n",
    "        if not m.lower() in dset_list: dset_list[m.lower()] = set([])\n",
    "        dset_list[m.lower()].add(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_dset_count = {x: len(dset_count[x]) for x in dset_count}\n",
    "_dset_count = pd.DataFrame.from_dict(_dset_count, orient=\"index\")\n",
    "_dset_count = _dset_count.reset_index()\n",
    "_dset_count.columns = [\"dsetnumber\", \"sharedschema\"]\n",
    "_dset_count.to_csv(\"plot_dsethist.tsv\", sep = \"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_sort = []\n",
    "for k in dset_count:\n",
    "    if k > 10: \n",
    "        for m in dset_count[k]: top_sort.append(m)\n",
    "\n",
    "ap = generate_prefix_list(top_sort)\n",
    "sortint_ap = sorted({k: len(ap[k]) for k in ap}.items(), key=itemgetter(1), reverse=True)\n",
    "for k in sortint_ap: print k[0] + '\\t' + str(k[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
